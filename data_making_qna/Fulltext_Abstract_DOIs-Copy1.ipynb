{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/ElsevierDev/elsapy/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml.etree.ElementTree â€” The ElementTree XML API\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some important elsevier api libraries\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsprofile import ElsAuthor, ElsAffil\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import xmltodict\n",
    "import urllib3\n",
    "import re\n",
    "\n",
    "#Import warnings library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = {'apikey': '22d18e0c4fdc59bb8898844056b2be57'}\n",
    "config = {\n",
    "    \"apikey\": \"2dc442325fc67f2f275ec3157ef8df65\",\n",
    " \"insttoken\": \"6beb1f6c29d85f50029bf11c8de94d1b\"\n",
    "    }\n",
    "\n",
    "client = ElsClient(config['apikey'])\n",
    "client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Science Direct Full Text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Access the avaiable full text DOIs\n",
    "# df0 = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/PE_RECYCLE_6000_withlink.csv')\n",
    "# print('Shape of data:',df0.shape)\n",
    "# #Shows the five rows of data frame\n",
    "# df0.head()\n",
    "\n",
    "# #Access the avaiable full text DOIs\n",
    "# df1 = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/PE_SYNTHESIS_6000_withlink.csv')\n",
    "# print('Shape of data:',df1.shape)\n",
    "# #Shows the five rows of data frame\n",
    "# df1.head()\n",
    "\n",
    "#Access the avaiable full text DOIs\n",
    "df2 = pd.read_csv(r\"/home/user3/Documents/avan_phd/Objective_3/Objective_3_QnA/data/elsevier_data/pet_recycle_methods.csv\")\n",
    "print('Shape of data:',df2.shape)\n",
    "#Shows the five rows of data frame\n",
    "df2.head()\n",
    "\n",
    "# #Access the avaiable full text DOIs\n",
    "# df3 = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/data_/PET_SYNTHESIS_6000_withlink.csv')\n",
    "# print('Shape of data:',df3.shape)\n",
    "# #Shows the five rows of data frame\n",
    "# df3.head()\n",
    "\n",
    "# #Access the avaiable full text DOIs\n",
    "# df4 = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/PP_RECYCLE_6000_withlink.csv')\n",
    "# print('Shape of data:',df4.shape)\n",
    "# #Shows the five rows of data frame\n",
    "# df4.head()\n",
    "\n",
    "# #Access the avaiable full text DOIs\n",
    "# df5 = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/PP_SYNTHESIS_6000_withlink.csv')\n",
    "# print('Shape of data:',df5.shape)\n",
    "# #Shows the five rows of data frame\n",
    "# df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for i in range(len(df2)):\n",
    "    date.append(int(df2.Year[i][:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(date).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df2)):\n",
    "    print(df2.Title[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([df2,df3], axis=0).reset_index(drop = True)\n",
    "# df.tail()\n",
    "df=  df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.reset_index(drop = True)\n",
    "print('Total index:',df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.drop_duplicates(keep='last').reset_index(drop = True)\n",
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append \"apikey\" and \"insttoken\" as suggest in the ElseSearch document into a config file\n",
    "config = {\n",
    "    \"apikey\": \"2dc442325fc67f2f275ec3157ef8df65\",\n",
    " \"insttoken\": \"6beb1f6c29d85f50029bf11c8de94d1b\"\n",
    "    }\n",
    "\n",
    "client = ElsClient(config['apikey'])\n",
    "client.inst_token = config['insttoken']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Create string1 and string2 to join doi with institoken to make a single URL\n",
    "string1 = \"https://api.elsevier.com/content/article/doi/\" \n",
    "string2 = \"?apiKey=2dc442325fc67f2f275ec3157ef8df65&insttoken=6beb1f6c29d85f50029bf11c8de94d1b\"\n",
    "\n",
    "#Access every DOI in the previous file and append the new URL to another column\n",
    "df_new['Link'] = df_new['DOI'].apply(lambda x: string1 + str(x) + string2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df_new\n",
    "df['Link'][50]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#The data loaded here with links\n",
    "df = pd.read_csv(r'/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Data extraction/doi_6000_withlink.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Link'][63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Random check the links** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": config['apikey'], \"X-ELS-Insttoken\": config['insttoken'], \"Accept\": \"application/xml\"}\n",
    "\n",
    "yes = 0\n",
    "no = 0\n",
    "for i in range(0, 2):\n",
    "    #x takes response of the HTTP request, passes link\n",
    "    x = requests.get(df['Link'][i], headers=headers_dict)\n",
    "    #print(x.text) #check\n",
    "        \n",
    "    #saving it as XML file\n",
    "    with open(\"full_text.xml\", 'wb') as f:\n",
    "        f.write(x.content)\n",
    "        \n",
    "    tree = ET.parse('full_text.xml')\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "    doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "    serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "    print(serial_item)\n",
    "\n",
    "    if serial_item != None:\n",
    "        yes +=1\n",
    "        print(yes)\n",
    "    else:\n",
    "        #print(\"Full text for this paper doesn't exist\")\n",
    "        no+=1\n",
    "        \n",
    "print(yes)\n",
    "print(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Extract Coredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://api.elsevier.com/content/article/doi/10.1016/j.molliq.2020.113766?apiKey=2dc442325fc67f2f275ec3157ef8df65&insttoken=6beb1f6c29d85f50029bf11c8de94d1b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coredata is present in every paper\n",
    "\n",
    "#Element.findall() finds only elements with a tag which are direct children of the current element\n",
    "#Element.find() finds the first child with a particular tag\n",
    "\n",
    "#defining a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": config['apikey'], \"X-ELS-Insttoken\": config['insttoken'], \"Accept\": \"application/xml\"}\n",
    "\n",
    "#x takes response of the HTTP request, random passes link\n",
    "x = requests.get(link, headers=headers_dict)\n",
    "print(x.text) #check\n",
    "    \n",
    "# #saving it as XML file\n",
    "# with open(\"full_text1.xml\", 'wb') as f:\n",
    "#     f.write(x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years =  []\n",
    "for i in range(df2.shape[0]):\n",
    "    \n",
    "    years.append(int(df2.Year[0][:4]))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsf(x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coredata is present in every paper\n",
    "\n",
    "#Element.findall() finds only elements with a tag which are direct children of the current element\n",
    "#Element.find() finds the first child with a particular tag\n",
    "\n",
    "#defining a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": config['apikey'], \"X-ELS-Insttoken\": config['insttoken'], \"Accept\": \"application/xml\"}\n",
    "\n",
    "#x takes response of the HTTP request, random passes link\n",
    "x = requests.get(df['Link'][30], headers=headers_dict)\n",
    "#print(x.text) #check\n",
    "    \n",
    "#saving it as XML file\n",
    "with open(\"full_text.xml\", 'wb') as f:\n",
    "    f.write(x.content)\n",
    "\n",
    "#Reading the data\n",
    "#<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "tree = ET.parse('full_text.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for entry in root.findall('{http://www.elsevier.com/xml/svapi/article/dtd}coredata'):\n",
    "    \n",
    "    url = []; doi =[]; title = []; pub_name =[]; type_ = []\n",
    "    description = []\n",
    "    #CHECK CODE\n",
    "    url.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}url').text)\n",
    "    doi.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}doi').text)\n",
    "    title.append(entry.find('{http://purl.org/dc/elements/1.1/}title').text)\n",
    "    pub_name.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}publicationName').text)\n",
    "    type_.append(entry.find('{http://prismstandard.org/namespaces/basic/2.0/}aggregationType').text)\n",
    "    description.append(entry.find('{http://purl.org/dc/elements/1.1/}description').text)\n",
    "    description[0] = \" \".join(description[0].split())\n",
    "    print('URL:',url )\n",
    "    print('DOI:', doi)\n",
    "    print('Title:', title)\n",
    "    print('Journal name:',pub_name )\n",
    "    print('Type:',type_ )\n",
    "    print('\\n')\n",
    "    print('Abstract:',description[0] )\n",
    "    print('\\n')\n",
    "    \n",
    "#Extract the list of authors and keywords mentioned      \n",
    "original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "#print(originaltext)\n",
    "doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "#print(doc)\n",
    "serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "#print(serial_item)\n",
    "\n",
    "keyword_list = []\n",
    "author_list = []\n",
    "if serial_item != None:\n",
    "    article = serial_item.find('{http://www.elsevier.com/xml/ja/dtd}article')\n",
    "    #print(article)\n",
    "    head = article.find('{http://www.elsevier.com/xml/ja/dtd}head')\n",
    "    #print(head)\n",
    "    author_group = head.find('{http://www.elsevier.com/xml/common/dtd}author-group')\n",
    "    #print(author_group)\n",
    "\n",
    "    keywords_ = head.find('{http://www.elsevier.com/xml/common/dtd}keywords')\n",
    "    #print(keywords_)\n",
    "\n",
    "    for author in author_group.findall('{http://www.elsevier.com/xml/common/dtd}author'):\n",
    "        name = author.find('{http://www.elsevier.com/xml/common/dtd}given-name').text\n",
    "        surname = author.find('{http://www.elsevier.com/xml/common/dtd}surname').text\n",
    "        author_list.append(name + ' ' + surname)\n",
    "        #print('\\n')\n",
    "\n",
    "    for word in keywords_.itertext():\n",
    "        keyword_list.append(word)\n",
    "\n",
    "    keyword_list = \"\".join(keyword_list)\n",
    "    keyword_list = list(keyword_list.split())\n",
    "    #keywords, authors present in the paper\n",
    "    print('Keywords present:',keyword_list)\n",
    "    print('Authors: ',author_list)\n",
    "else:\n",
    "    print(\"Full text for this paper doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract full text and store in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#defining a header's dictionary to pass through requests\n",
    "headers_dict = {\"X-ELS-APIKey\": config['apikey'], \"X-ELS-Insttoken\": config['insttoken'], \"Accept\": \"application/xml\"}\n",
    "\n",
    "df_temp = pd.DataFrame()\n",
    "#excel_writer = pd.ExcelWriter(r'C:\\Users\\Admin\\OneDrive - IIT Delhi\\CPCB\\PROF. HARI\\test2.xlsx', engine='xlsxwriter')\n",
    "for i in np.arange(4933,df.shape[0],1):\n",
    "    print(i, 'done' )\n",
    "    #x takes response of the HTTP request, passes link\n",
    "    x = requests.get(df['Link'][i], headers=headers_dict)\n",
    "\n",
    "    #saving it as XML file\n",
    "    with open(\"full_text.xml\", 'wb') as f:\n",
    "        f.write(x.content)\n",
    "\n",
    "    #Reading the data\n",
    "    #<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    tree = ET.parse('full_text.xml')\n",
    "    root = tree.getroot()\n",
    "    original_text = root.find('{http://www.elsevier.com/xml/svapi/article/dtd}originalText')\n",
    "    \n",
    "    if original_text != None:\n",
    "        doc = original_text.find('{http://www.elsevier.com/xml/xocs/dtd}doc')\n",
    "        serial_item = doc.find('{http://www.elsevier.com/xml/xocs/dtd}serial-item')\n",
    "\n",
    "        if serial_item != None:\n",
    "            article = serial_item.find('{http://www.elsevier.com/xml/ja/dtd}article')\n",
    "            \n",
    "            if article != None:\n",
    "                body = article.find('{http://www.elsevier.com/xml/ja/dtd}body')\n",
    "                \n",
    "                if body != None:\n",
    "                    sections = body.find('{http://www.elsevier.com/xml/common/dtd}sections')\n",
    "                    \n",
    "                    #lists for dataframe\n",
    "                    label_list = [] \n",
    "                    title_list = []\n",
    "                    para_list = []\n",
    "\n",
    "                    for section1 in sections.findall('{http://www.elsevier.com/xml/common/dtd}section'):\n",
    "                        if section1.find('{http://www.elsevier.com/xml/common/dtd}label') != None:\n",
    "                        #if section1 != None:\n",
    "                            label = section1.find('{http://www.elsevier.com/xml/common/dtd}label').text\n",
    "                            label_list.append(label)\n",
    "                            section_title_list = []\n",
    "                            section_title = section1.find('{http://www.elsevier.com/xml/common/dtd}section-title')\n",
    "                            for sec in section_title.itertext():\n",
    "                                section_title_list.append(sec)\n",
    "\n",
    "                            section_title_list = \"\".join(section_title_list)\n",
    "                            section_title_list = list(section_title_list.split()) \n",
    "                            section_title_list = \" \".join(section_title_list)\n",
    "                            title_list.append(section_title_list)\n",
    "                            print(section_title_list)\n",
    "\n",
    "                            total_para = []\n",
    "                            for paragraph in section1.findall('{http://www.elsevier.com/xml/common/dtd}para'):\n",
    "                                if paragraph != None:\n",
    "                                    paragraph1 = []\n",
    "                                    for p in paragraph.itertext():\n",
    "                                        paragraph1.append(p)\n",
    "\n",
    "                                    paragraph1 = \"\".join(paragraph1)\n",
    "                                    paragraph1 = list(paragraph1.split()) \n",
    "                                    paragraph1 = \" \".join(paragraph1)\n",
    "                                    total_para.append(paragraph1)\n",
    "\n",
    "                            total_para = \" \".join(total_para)\n",
    "                            #print(total_para)\n",
    "                            para_list.append(total_para)\n",
    "                            print('\\n')\n",
    "\n",
    "                            for section2 in section1.findall('{http://www.elsevier.com/xml/common/dtd}section'):\n",
    "                                if section2.find('{http://www.elsevier.com/xml/common/dtd}label') !=None:\n",
    "\n",
    "                                    label = section2.find('{http://www.elsevier.com/xml/common/dtd}label').text\n",
    "\n",
    "                                    label_list.append(label)\n",
    "                                    print('sub-section ', label)\n",
    "\n",
    "                                    sub_section_title_list = []\n",
    "\n",
    "                                    sub_section_title = section2.find('{http://www.elsevier.com/xml/common/dtd}section-title')\n",
    "                                    for sub in sub_section_title.itertext():\n",
    "                                        sub_section_title_list.append(sub)\n",
    "\n",
    "                                    sub_section_title_list = \"\".join(sub_section_title_list)\n",
    "                                    sub_section_title_list = list(sub_section_title_list.split()) \n",
    "                                    sub_section_title_list = \" \".join(sub_section_title_list)\n",
    "                                    title_list.append(sub_section_title_list)\n",
    "                                    #print('sub-section-title: ', sub_section_title_list)\n",
    "\n",
    "                                    para1 = []\n",
    "                                    for para in section2.itertext():\n",
    "                                    #findall('{http://www.elsevier.com/xml/common/dtd}para'):\n",
    "                                        #paragraph = para.\n",
    "                                        #for i in paragraph: \n",
    "                                        para1.append(para)\n",
    "\n",
    "                                    para1 = \"\".join(para1)\n",
    "                                    para1 = list(para1.split()) \n",
    "                                    para1 = \" \".join(para1)\n",
    "                                    #print(para1) \n",
    "                                    para_list.append(para1)\n",
    "                                    print('\\n')\n",
    "\n",
    "#     temp = dict(doi = np.array(doi), \n",
    "#          title = np.array(title), \n",
    "#          pub_name = np.array(pub_name), \n",
    "#          Type = np.array(type_), \n",
    "#          abstract= np.array(description), \n",
    "#          authors= np.array(author_list), \n",
    "#          keyword =np.array(keyword_list),\n",
    "#          title_label = np.array(label_list),\n",
    "#          titles = np.array(title_list),\n",
    "#          text = np.array(para_list))\n",
    "    temp = dict(titles = np.array(title_list), \n",
    "          text = np.array(para_list)) \n",
    "    \n",
    "    #print(len(temp['text']), type(temp['text']))\n",
    "    #print(len(temp['titles']), type(temp['titles']))\n",
    "    \n",
    "    df_temp = pd.DataFrame({'sections' : temp['titles'].tolist(), 'Text': temp['text'].tolist()})\n",
    "    \n",
    "    #df_temp.head()\n",
    "    \n",
    "    #Save each data frame\n",
    "    df_temp.to_csv('/home/user3/Documents/avan_phd/Objective_3/Ex_scibert-main/Raw_article_data/data/{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'/home/user3/Documents/avan_phd/objective_escape/data/2022.csv')\n",
    "# df.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "list_abstract = []\n",
    "list_doi= []\n",
    "list_title = []\n",
    "list_date = []\n",
    "list_journal = []\n",
    "\n",
    "## ScienceDirect (full-text) document example using DOI\n",
    "#for i in range(len(data)):\n",
    "from pprint import pprint\n",
    "doi= df['DOI']\n",
    "for i in range(df.shape[0]):\n",
    "    pprint(doi[i])\n",
    "    doi_doc = FullDoc(doi = doi[i])\n",
    "    if doi_doc.read(client):\n",
    "\n",
    "        #pprint(dir(doi_doc))\n",
    "        #pprint(doi_doc._data)\n",
    "        abstract = doi_doc._data['coredata']['dc:description']\n",
    "        title = doi_doc.title\n",
    "        date = doi_doc._data['coredata']['prism:coverDisplayDate']\n",
    "        journal = doi_doc._data['coredata']['prism:publicationName']\n",
    "        #pprint(abstract)\n",
    "        print(i, 'done')\n",
    "        #dict_abstract['DOI'] = doi[i]\n",
    "        #dict_abstract['Abstract'] = abstract\n",
    "        list_abstract.append(abstract)\n",
    "        list_doi.append(doi[i])\n",
    "        list_title.append(title)\n",
    "        list_date.append(date)\n",
    "        list_journal.append(journal)\n",
    "        #doi_doc.write()\n",
    "    \n",
    "    else:\n",
    "        pprint('Operation failed')\n",
    "        #dict_abstract['DOI'] = doi[i]\n",
    "        #dict_abstract['Abstract'] = 'NAN'\n",
    "        list_abstract.append('0')\n",
    "        list_doi.append('1')\n",
    "        list_title.append('2')\n",
    "        list_date.append('3')\n",
    "        list_journal.append('4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available_abstract = pd.DataFrame()\n",
    "df_available_abstract['DOI'] = list_doi\n",
    "df_available_abstract['Title'] = list_title\n",
    "df_available_abstract['Abstract'] = list_abstract\n",
    "df_available_abstract['Date'] = list_date\n",
    "df_available_abstract['Journal'] = list_journal\n",
    "\n",
    "df_available_abstract.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (df_available_abstract.DOI == \"1\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available_abstract.to_csv(r'/home/user3/Documents/avan_phd/Objective_3/Objective_3_QnA/data/pe_recycles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available_abstract.Title[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_available_abstract.Abstract[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
