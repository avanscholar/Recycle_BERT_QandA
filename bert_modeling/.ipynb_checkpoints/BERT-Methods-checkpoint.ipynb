{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT SOME BASIC LIBRARIES \n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e5caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data load\n",
    "with open(r\"meth_full_1351.json\", \"r\") as read_file:\n",
    "    full = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of data :\", len(full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into three categories\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, _ = train_test_split(full, test_size=0.30, random_state=42)\n",
    "valid, test = train_test_split(_, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f907f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79043019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataa and foramting the data \n",
    "def read_data(squad):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for group in squad:\n",
    "        context = group['context']\n",
    "        for qa in group['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "#Call the functions\n",
    "train_contexts, train_questions, train_answers = read_data(train)\n",
    "valid_contexts, valid_questions, valid_answers = read_data(valid)\n",
    "test_contexts, test_questions, test_answers = read_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_contexts), len(train_answers)\n",
    "\n",
    "print(\"shape of train:>>\", len(train_contexts))\n",
    "print(\"shape of valid:>>\", len(valid_contexts))\n",
    "print(\"shape of test:>>\", len(test_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f43bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts[1], train_questions[1], train_answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two so we fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "        else:\n",
    "            answer['answer_end'] = end_idx\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(valid_answers, valid_contexts)\n",
    "add_end_idx(test_answers, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that now we get the answer_end also\n",
    "print(train_questions[-10])\n",
    "print(train_answers[-10])\n",
    "print(train_contexts[-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f445b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_encodings = len(valid_encodings['input_ids'])\n",
    "print(f'We have {no_of_encodings} context-question pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_encodings = len(test_encodings['input_ids'])\n",
    "print(f'We have {no_of_encodings} context-question pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_encodings = len(train_encodings['input_ids'])\n",
    "print(f'We have {no_of_encodings} context-question pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        # print(i)\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # print(i)\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "        encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(valid_encodings, valid_answers)\n",
    "add_token_positions(test_encodings, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings['start_positions'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bc023",
   "metadata": {},
   "source": [
    "# Dataset definition üóÑÔ∏è\n",
    "We have to define our dataset using the PyTorch Dataset class from torch.utils in order create our dataloaders after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuAD_Dataset(train_encodings)\n",
    "valid_dataset = SQuAD_Dataset(valid_encodings)\n",
    "test_dataset  = SQuAD_Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60198791",
   "metadata": {},
   "source": [
    "**Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48505d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab695ff",
   "metadata": {},
   "source": [
    " **Load pretrained BERT**\n",
    " \n",
    "We are going to use the bert-case-uncased from the huggingface transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a84a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "# model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the available device - use GPU\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec54f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-cased').to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "# optim = AdamW(model.parameters(), lr=3e-5)\n",
    "# optim = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# epochs = 2\n",
    "epochs = 10\n",
    "# epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_eval_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Set model in train mode\n",
    "    model.train()\n",
    "\n",
    "    loss_of_epoch = 0\n",
    "\n",
    "    print(\"############Train############\")\n",
    "\n",
    "    for batch_idx,batch in enumerate(train_loader): \n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        # do a backwards pass \n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optim.step()\n",
    "        # Find the total loss\n",
    "        loss_of_epoch += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % print_every == 0:\n",
    "            print(\"Batch {:} / {:}\".format(batch_idx+1,len(train_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
    "\n",
    "    loss_of_epoch /= len(train_loader)\n",
    "    train_losses.append(loss_of_epoch)\n",
    "\n",
    "    ##########Evaluation##################\n",
    "\n",
    "    # Set model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print(\"############Evaluate############\")\n",
    "\n",
    "    loss_of_epoch = 0\n",
    "\n",
    "    for batch_idx,batch in enumerate(val_loader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs[0]\n",
    "            # Find the total loss\n",
    "            loss_of_epoch += loss.item()\n",
    "\n",
    "        if (batch_idx+1) % print_every == 0:\n",
    "            print(\"Batch {:} / {:}\".format(batch_idx+1,len(val_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
    "\n",
    "    loss_of_epoch /= len(val_loader)\n",
    "    val_losses.append(loss_of_epoch)\n",
    "\n",
    "    # Print each epoch's time and train/val loss \n",
    "    print(\"\\n-------Epoch \", epoch+1,\n",
    "        \"-------\"\n",
    "        \"\\nTraining Loss:\", train_losses[-1],\n",
    "        \"\\nValidation Loss:\", val_losses[-1],\n",
    "        \"\\nTime: \",(time.time() - epoch_time),\n",
    "        \"\\n-----------------------\",\n",
    "        \"\\n\\n\")\n",
    "\n",
    "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6f202",
   "metadata": {},
   "source": [
    "**Save the model in my drive in order not to run it each time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = PATH  + \"/meth_only/\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ea0dd",
   "metadata": {},
   "source": [
    "# Step 12: Plot train and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses, color='red', label= 'Train', linewidth=2.5)\n",
    "plt.plot(range(len(val_losses)), val_losses, color =  'Blue', label = 'Validation', linewidth=2.5)\n",
    "#plt.title('Classification loss')\n",
    "plt.legend(frameon=False,prop={'weight':'bold',\"size\":16})\n",
    "plt.xlabel('Number of epoch',fontweight='bold', fontsize=18)\n",
    "plt.ylabel('Loss',fontweight='bold', fontsize=18)\n",
    "#plt.title('GP Regression',fontweight='bold')\n",
    "#plt.axis('square')\n",
    "from matplotlib import rc\n",
    "\n",
    "plt.rcParams['axes.linewidth'] = 3\n",
    "\n",
    "plt.tick_params(axis=\"x\", direction=\"in\",width=4)\n",
    "plt.tick_params(axis=\"y\", direction=\"in\", width=4)\n",
    "\n",
    "rc('font', weight='bold')\n",
    "\n",
    "plt.tick_params(bottom=True, top=True, left=True, right=True)\n",
    "plt.tick_params(labelbottom=True, labeltop=False, labelleft=True, labelright=False)\n",
    "plt.xticks(rotation = '0', fontsize = 14)\n",
    "plt.yticks(rotation = '0', fontsize = 14)\n",
    "#plt.legend(handles=h, labels=np.arange(0.9,0.85), title=\"Quality\")\n",
    "#plt.rcParams.update({'legend.fontweight':'bold'}\n",
    "#plt.savefig('line_plot.pdf')\n",
    "plt.savefig(r'/home/user3/Documents/avan_phd/Objective_3/Objective_3_QnA/result/methods/curve_learnin/learning_react.pdf', dpi=5000)\n",
    "plt.show()#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9047a",
   "metadata": {},
   "source": [
    "# Test\n",
    "We are evaluating the model on the validation set by checking the model's predictions for the answer's start and end indexes and comparing with the true ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5662f",
   "metadata": {},
   "source": [
    "# Data load for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba3c07",
   "metadata": {},
   "source": [
    "**Respectively, load the saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa196b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "#Load the pretrained weights\n",
    "model_path =PATH + \"/meth_only/\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "#Specify the cuda\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Working on {device}')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##data list\n",
    "context = []\n",
    "answerss = []\n",
    "question =[]\n",
    "\n",
    "#loop data points\n",
    "def extractquestions(test):\n",
    "    for trai in test:\n",
    "        context.append(trai['context'])\n",
    "        question.append(trai['qas'][0]['question'])\n",
    "\n",
    "    for sample in test:\n",
    "        gold_answers = []\n",
    "        for ans in sample['qas'][0]['answers']:\n",
    "            gold_answers.append(ans['text'])\n",
    "        answerss.append(gold_answers)\n",
    "        \n",
    "    return context, answerss, question\n",
    "\n",
    "#Data for testing\n",
    "context, answerss, question = extractquestions(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the metric lists\n",
    "F1 = []\n",
    "preci = []\n",
    "recal = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_f1_0(context, question, f):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "    try:\n",
    "        outputs = model(**inputs)\n",
    "        start_logits = outputs[0]\n",
    "        end_logits = outputs[1]\n",
    "\n",
    "        def to_list(tensor):\n",
    "            return tensor.detach().cpu().tolist()\n",
    "\n",
    "        # convert our start and end logit tensors to lists\n",
    "        start_logits = to_list(start_logits)[0]\n",
    "        end_logits = to_list(end_logits)[0]\n",
    "\n",
    "        # sort our start and end logits from largest to smallest, keeping track of the index\n",
    "        start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "        end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        start_indexes = [idx for idx, logit in start_idx_and_logit[:5]]\n",
    "        end_indexes = [idx for idx, logit in end_idx_and_logit[:5]]\n",
    "\n",
    "        # convert the token ids from a tensor to a list\n",
    "        tokens = to_list(inputs['input_ids'])[0]\n",
    "\n",
    "        # question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token \n",
    "        question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "\n",
    "        import collections\n",
    "\n",
    "        # keep track of all preliminary predictions\n",
    "        PrelimPrediction = collections.namedtuple( \n",
    "            \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "        )\n",
    "\n",
    "        prelim_preds = []\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # throw out invalid predictions\n",
    "                if start_index in question_indexes:\n",
    "                    continue\n",
    "                if end_index in question_indexes:\n",
    "                    continue\n",
    "                if end_index < start_index:\n",
    "                    continue\n",
    "                prelim_preds.append(\n",
    "                    PrelimPrediction(\n",
    "                        start_index = start_index,\n",
    "                        end_index = end_index,\n",
    "                        start_logit = start_logits[start_index],\n",
    "                        end_logit = end_logits[end_index]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # sort preliminary predictions by their score\n",
    "        prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "\n",
    "        # keep track of all best predictions\n",
    "        BestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "        )\n",
    "\n",
    "        nbest = []\n",
    "        nbest_text =[]\n",
    "        seen_predictions = []\n",
    "        print(\"best answers: \",f)\n",
    "        for pred in prelim_preds:\n",
    "\n",
    "            # for now we only care about the top 5 best predictions\n",
    "            if len(nbest) >= f: \n",
    "                break\n",
    "\n",
    "            # loop through predictions according to their start index\n",
    "            if pred.start_index > 0: # non-null answers have start_index > 0\n",
    "\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.convert_ids_to_tokens(\n",
    "                        tokens[pred.start_index:pred.end_index+1]\n",
    "                    )\n",
    "                )\n",
    "                # clean whitespace\n",
    "                text = text.strip()\n",
    "                text = \" \".join(text.split())\n",
    "\n",
    "                if text in seen_predictions:\n",
    "                    continue\n",
    "\n",
    "                # flag this text as being seen -- if we see it again, don't add it to the nbest list\n",
    "                seen_predictions.append(text) \n",
    "\n",
    "                # add this text prediction to a pruned list of the top 5 best predictions\n",
    "                nbest.append(BestPrediction(text=text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
    "\n",
    "        # and don't forget -- include the null answer!\n",
    "        nbest.append(BestPrediction(text=\"\", start_logit=start_logits[0], end_logit=end_logits[0]))\n",
    "\n",
    "    except:\n",
    "        print('no available')\n",
    "        \n",
    "        seen_predictions = [\"nothings\"]\n",
    "\n",
    "    return seen_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, truth):\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ae500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(context, question,answerss,f):\n",
    "    prediction = get_prediction_f1_0(context,question,f)\n",
    "    #em_score = exact_match(prediction, answer)\n",
    "    #f1_score = compute_f1(prediction, answer)\n",
    "    temp_f1 = []\n",
    "    for pre in prediction:\n",
    "        \n",
    "        print(answerss, pre)\n",
    "\n",
    "        if len(answerss) != 0:\n",
    "            em_score = max((exact_match(pre.replace(\" \", \"\"), answer.replace(\" \", \"\"))) for answer in answerss)\n",
    "\n",
    "            f1_score = max((compute_f1(pre.replace(\" \", \"\"), answer.replace(\" \", \"\"))) for answer in answerss)\n",
    "            print(f1_score)\n",
    "    #         if f1_score == 1.0:\n",
    "            temp_f1.append(f1_score)\n",
    "    \n",
    "        else:\n",
    "            em_score = 0\n",
    "            f1_score = 0\n",
    "            temp_f1 = [0]\n",
    "            \n",
    "    #if max(temp_f1) <=1.0:\n",
    "        \n",
    "    if max(temp_f1) ==1.0:\n",
    "        F1.append(max(temp_f1))\n",
    "        \n",
    "    else:\n",
    "        temp_f1 = []\n",
    "        for pre in prediction:\n",
    "\n",
    "            print(answerss, pre)\n",
    "\n",
    "            if len(answerss) != 0:\n",
    "                em_score = max((exact_match(pre, answer)) for answer in answerss)\n",
    "        #         for answer in answerss:\n",
    "        #             presion, recall = comput_prec_recall(prediction, answer.replace(\" \", \"\"))\n",
    "        # #         presion, recall = max((comput_prec_recall(prediction, answer)) for answer in answerss)\n",
    "        #         print(presion, recall)\n",
    "\n",
    "                f1_score = max((compute_f1(pre, answer)) for answer in answerss)\n",
    "                print(f1_score)\n",
    "        #         if f1_score == 1.0:\n",
    "                temp_f1.append(f1_score)\n",
    "            else:\n",
    "                em_score = 0\n",
    "                f1_score = 0\n",
    "                temp_f1 = [0]\n",
    "                \n",
    "        F1.append(max(temp_f1))\n",
    "\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Prediction: {prediction}')\n",
    "    print(f'True Answer: {answerss}')\n",
    "    print(f'Exact match: {em_score}')\n",
    "    print(f'F1 score: {max(temp_f1)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full)):\n",
    "    print(i)\n",
    "    question_answer(context[i], question[0],answerss[i], len(answerss[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(F1), len(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e39787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(F1)/len(F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart11",
   "language": "python",
   "name": "bart11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
